version: "3.0"
volumes:
  shared-workspace:
    name: "hadoop-distributed-file-system"
    driver: local

services:
  zeppelin_notebook_server:
    image: zeppelin
    container_name: zeppelin_notebook_server
    restart: unless-stopped
    volumes:
      - ./config-zeppelin:/zeppelin/conf:rw
#      - ./zeppelin-config/interpreter.json:/zeppelin/conf/interpreter.json:rw
      - ./notebooks-zeppelin:/zeppelin/notebook
      - ./input-data:/input-data:ro
      - ./output-data:/output-data:rw
      - shared-workspace:/opt/workspace
    ports:
      - "8085:8080"
    labels:
      container_group: "notebook"
    environment:
      MASTER: "spark://spark-master:7077"
      SPARK_MASTER: "spark://spark-master:7077"
# set via interpreter.json setting file in ./zeppelin-config
#      SPARK_HOME: /usr/bin/spark-3.0.0-bin-hadoop2.7
#      SPARK_HOME: /usr/spark-2.4.1
    depends_on:
      - spark-master

  spark-master:
    image: spark-master
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
# Experimental see: https://spark.apache.org/docs/3.0.0-preview2/job-scheduling.html
    environment:
      PYSPARK_PIN_THREAD: 'true'
    volumes:
      - shared-workspace:/opt/workspace

  spark-worker-1:
    image: spark-worker
    container_name: spark-worker-1
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1132m
# Experimental see: https://spark.apache.org/docs/3.0.0-preview2/job-scheduling.html
      - PYSPARK_PIN_THREAD=true
    ports:
      - 8081:8081
    volumes:
      - shared-workspace:/opt/workspace
    depends_on:
      - spark-master

#  spark-worker-2:
#    image: spark-worker
#    container_name: spark-worker-2
#    environment:
#      - SPARK_WORKER_CORES=1
#      - SPARK_WORKER_MEMORY=512m
#    ports:
#      - 8082:8081
#    volumes:
#      - shared-workspace:/opt/workspace
#    depends_on:
#      - spark-master

